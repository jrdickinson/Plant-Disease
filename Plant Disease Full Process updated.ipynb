{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ANN, No Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Flatten(input_shape=(shape,shape,3)))\n",
    "model.add(Dense(128, kernel_initializer=\"glorot_normal\"))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dropout(.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GaussianNoise(1))\n",
    "model.add(Dense(128))\n",
    "model.add(LeakyReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(38, activation='softmax'))\n",
    "# model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "model.compile(optimizer=noisy(),\n",
    "# model.compile(optimizer=\"adam\",\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=[categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dir = \"C:/Project/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train\"\n",
    "test_dir = \"C:/Project/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/valid\"\n",
    "# train_dir = \"C:/Project/train/train\"\n",
    "# test_dir = \"C:/Project/valid/valid\"\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=False, zca_epsilon=.2, width_shift_range=0, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    #use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "# model = load_model('best_model.h5')\n",
    "model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ANN, Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "batch_size = 128\n",
    "train_dir = \"C:/Project/64/train\"\n",
    "test_dir = \"C:/Project/64/valid\"\n",
    "# train_dir = \"C:/Project/train/train\"\n",
    "# test_dir = \"C:/Project/valid/valid\"\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 1\n",
    "\n",
    "base_model=VGG16(weights=\"imagenet\", include_top=False, input_shape=(shape,shape,3))\n",
    "x= base_model.output\n",
    "x = Flatten()(x)\n",
    "x= (Dense(128, kernel_initializer=\"glorot_normal\"))(x)\n",
    "x = (LeakyReLU())(x)\n",
    "x = (Dropout(.2))(x)\n",
    "x = (BatchNormalization())(x)\n",
    "x = (GaussianNoise(1))(x)\n",
    "x = (Dense(128))(x)\n",
    "x = (LeakyReLU())(x)\n",
    "x = (BatchNormalization())(x)\n",
    "pred = (Dense(38, activation='softmax'))(x)\n",
    "model = Model(inputs=base_model.input, outputs=pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "model.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cnn Model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "batch_size = 128   ###############################################################################\n",
    "train_dir = \"C:/Project/64/train\" ###############################################################\n",
    "test_dir = \"C:/Project/64/valid\"  ##################################################################\n",
    "# train_dir = \"C:/Project/train/train\"\n",
    "# test_dir = \"C:/Project/valid/valid\"\n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "# base_model=VGG16(weights=\"imagenet\", include_top=False, input_shape=(shape,shape,3))\n",
    "# x= base_model.output\n",
    "# x = Flatten()(x)\n",
    "# x= (Dense(128, kernel_initializer=\"glorot_normal\"))(x)\n",
    "# x = (LeakyReLU())(x)\n",
    "# x = (Dropout(.2))(x)\n",
    "# x = (BatchNormalization())(x)\n",
    "# x = (GaussianNoise(1))(x)\n",
    "# x = (Dense(128))(x)\n",
    "# x = (LeakyReLU())(x)\n",
    "# x = (BatchNormalization())(x)\n",
    "# pred = (Dense(38, activation='softmax'))(x)\n",
    "# model = Model(inputs=base_model.input, outputs=pred)\n",
    "\n",
    "\n",
    "\n",
    "# x = models.Sequential()\n",
    "# x.add(Dense(128, kernel_initializer=\"glorot_normal\", input_shape = (shape,shape,3)))\n",
    "# x.add(Flatten())\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(Dropout(.2))\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(GaussianNoise(1))\n",
    "# x.add(Dense(128))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Dense(4, activation='softmax')) ################################################################\n",
    "\n",
    "x = models.Sequential()\n",
    "\n",
    "x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "# x.add(Dropout(.2))\n",
    "\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "# x.add(Dropout(.2))\n",
    "\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# # x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# # x.add(LeakyReLU())\n",
    "# # x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "# x.add(Dropout(.2))\n",
    "\n",
    "x.add(Flatten())\n",
    "x.add(Dense(64, kernel_initializer=\"glorot_normal\"))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "# x.add(Dropout(.4))\n",
    "x.add(Dense(38, activation='softmax')) ################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "x.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = x.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "batch_size = 128   \n",
    "train_dir = \"C:/Project/64/train\" \n",
    "test_dir = \"C:/Project/64/valid\"  \n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = models.Sequential()\n",
    "\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "# x.add(Dropout(.2))\n",
    "\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "# x.add(Dropout(.2))\n",
    "\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# # x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# # x.add(LeakyReLU())\n",
    "# # x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "# x.add(Dropout(.2))\n",
    "\n",
    "x.add(Flatten())\n",
    "x.add(Dense(256, kernel_initializer=\"glorot_normal\"))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "# x.add(Dropout(.4))\n",
    "x.add(Dense(38, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "x.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = x.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "batch_size = 128   \n",
    "train_dir = \"C:/Project/64/train\" \n",
    "test_dir = \"C:/Project/64/valid\"  \n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = models.Sequential()\n",
    "\n",
    "x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Flatten())\n",
    "x.add(Dense(128, kernel_initializer=\"glorot_normal\"))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "\n",
    "x.add(Dense(38, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "x.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "# train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "#                                                    target_size=(shape,shape),\n",
    "#                                                    color_mode=\"rgb\",\n",
    "#                                                    batch_size=batch_size,\n",
    "#                                                    class_mode=\"categorical\",\n",
    "#                                                    shuffle=True)\n",
    "# test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "#                                                     target_size=(shape,shape),\n",
    "#                                                     color_mode=\"rgb\",\n",
    "#                                                     batch_size=batch_size,\n",
    "#                                                     class_mode=\"categorical\"\n",
    "#                                                     )\n",
    "# history = x.fit_generator(train_generator,\n",
    "#                     steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "#                     epochs=epochs,\n",
    "#                     validation_data=test_generator,\n",
    "#                     validation_steps=nb_test_samples // batch_size+1,\n",
    "#                     workers=4,\n",
    "#                     callbacks=callbacks\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.571805825191127, 0.8235829729657225]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "batch_size = 128   \n",
    "train_dir = \"C:/Project/64/train\" \n",
    "test_dir = \"C:/Project/64/valid\"  \n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = models.Sequential()\n",
    "\n",
    "x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Flatten())\n",
    "x.add(Dense(128, kernel_initializer=\"glorot_normal\"))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "\n",
    "x.add(Dense(38, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "x.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = x.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17572 images belonging to 38 classes.\n",
      "                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                Apple___Apple_scab       0.75      0.81      0.77       504\n",
      "                                 Apple___Black_rot       0.68      0.90      0.77       497\n",
      "                          Apple___Cedar_apple_rust       0.74      0.83      0.78       440\n",
      "                                   Apple___healthy       0.74      0.78      0.76       502\n",
      "                               Blueberry___healthy       0.75      0.83      0.79       454\n",
      "          Cherry_(including_sour)___Powdery_mildew       0.87      0.80      0.83       421\n",
      "                 Cherry_(including_sour)___healthy       0.80      0.95      0.87       456\n",
      "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot       0.79      0.71      0.74       410\n",
      "                       Corn_(maize)___Common_rust_       0.91      0.96      0.93       477\n",
      "               Corn_(maize)___Northern_Leaf_Blight       0.85      0.88      0.87       477\n",
      "                            Corn_(maize)___healthy       0.96      0.99      0.97       465\n",
      "                                 Grape___Black_rot       0.90      0.65      0.75       472\n",
      "                      Grape___Esca_(Black_Measles)       0.78      0.91      0.84       480\n",
      "        Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       0.89      0.97      0.93       430\n",
      "                                   Grape___healthy       0.79      0.90      0.84       423\n",
      "          Orange___Haunglongbing_(Citrus_greening)       0.83      0.93      0.88       503\n",
      "                            Peach___Bacterial_spot       0.82      0.76      0.79       459\n",
      "                                   Peach___healthy       0.73      0.97      0.84       432\n",
      "                     Pepper,_bell___Bacterial_spot       0.67      0.80      0.73       478\n",
      "                            Pepper,_bell___healthy       0.70      0.70      0.70       497\n",
      "                             Potato___Early_blight       0.85      0.92      0.88       485\n",
      "                              Potato___Late_blight       0.83      0.61      0.70       485\n",
      "                                  Potato___healthy       0.84      0.80      0.82       456\n",
      "                               Raspberry___healthy       0.90      0.80      0.85       445\n",
      "                                 Soybean___healthy       0.86      0.81      0.84       505\n",
      "                           Squash___Powdery_mildew       0.94      0.82      0.88       434\n",
      "                          Strawberry___Leaf_scorch       0.92      0.84      0.88       444\n",
      "                              Strawberry___healthy       0.88      0.84      0.86       456\n",
      "                           Tomato___Bacterial_spot       0.88      0.70      0.78       425\n",
      "                             Tomato___Early_blight       0.54      0.65      0.59       480\n",
      "                              Tomato___Late_blight       0.72      0.53      0.61       463\n",
      "                                Tomato___Leaf_Mold       0.82      0.64      0.72       470\n",
      "                       Tomato___Septoria_leaf_spot       0.69      0.41      0.52       436\n",
      "     Tomato___Spider_mites Two-spotted_spider_mite       0.67      0.79      0.73       435\n",
      "                              Tomato___Target_Spot       0.70      0.63      0.66       457\n",
      "            Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.91      0.79      0.85       490\n",
      "                      Tomato___Tomato_mosaic_virus       0.86      0.96      0.91       448\n",
      "                                  Tomato___healthy       0.91      0.86      0.88       481\n",
      "\n",
      "                                         micro avg       0.80      0.80      0.80     17572\n",
      "                                         macro avg       0.81      0.80      0.80     17572\n",
      "                                      weighted avg       0.81      0.80      0.80     17572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "# model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=False\n",
    "                                                    )\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())   \n",
    "\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "set_random_seed(1724)\n",
    "shape=64\n",
    "\n",
    "batch_size = 128   \n",
    "train_dir = \"C:/Project/64/train\" \n",
    "test_dir = \"C:/Project/64/valid\"  \n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "\n",
    "# base_model=VGG16(weights=\"imagenet\", include_top=False, input_shape=(shape,shape,3))\n",
    "# base_model.summary()\n",
    "# base_model = InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "# base_model.summary()\n",
    "\n",
    "\n",
    "x = models.Sequential()\n",
    "\n",
    "x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x.add(Flatten())\n",
    "x.add(Dense(1024, kernel_initializer=\"glorot_normal\"))\n",
    "x.add(LeakyReLU())\n",
    "x.add(BatchNormalization())\n",
    "\n",
    "\n",
    "x.add(Dense(38, activation='softmax')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "x.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = x.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17572 images belonging to 38 classes.\n",
      "                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                Apple___Apple_scab       0.84      0.79      0.81       504\n",
      "                                 Apple___Black_rot       0.88      0.85      0.87       497\n",
      "                          Apple___Cedar_apple_rust       0.91      0.84      0.87       440\n",
      "                                   Apple___healthy       0.90      0.79      0.84       502\n",
      "                               Blueberry___healthy       0.94      0.78      0.85       454\n",
      "          Cherry_(including_sour)___Powdery_mildew       0.86      0.95      0.90       421\n",
      "                 Cherry_(including_sour)___healthy       0.89      0.97      0.93       456\n",
      "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot       0.90      0.77      0.83       410\n",
      "                       Corn_(maize)___Common_rust_       0.94      0.99      0.96       477\n",
      "               Corn_(maize)___Northern_Leaf_Blight       0.90      0.91      0.91       477\n",
      "                            Corn_(maize)___healthy       0.99      0.99      0.99       465\n",
      "                                 Grape___Black_rot       0.87      0.86      0.87       472\n",
      "                      Grape___Esca_(Black_Measles)       0.90      0.90      0.90       480\n",
      "        Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       0.94      0.96      0.95       430\n",
      "                                   Grape___healthy       0.97      0.93      0.95       423\n",
      "          Orange___Haunglongbing_(Citrus_greening)       0.92      0.96      0.94       503\n",
      "                            Peach___Bacterial_spot       0.85      0.87      0.86       459\n",
      "                                   Peach___healthy       0.94      0.92      0.93       432\n",
      "                     Pepper,_bell___Bacterial_spot       0.64      0.91      0.75       478\n",
      "                            Pepper,_bell___healthy       0.71      0.82      0.76       497\n",
      "                             Potato___Early_blight       0.94      0.96      0.95       485\n",
      "                              Potato___Late_blight       0.81      0.87      0.84       485\n",
      "                                  Potato___healthy       0.90      0.86      0.88       456\n",
      "                               Raspberry___healthy       0.82      0.94      0.88       445\n",
      "                                 Soybean___healthy       0.99      0.72      0.83       505\n",
      "                           Squash___Powdery_mildew       0.91      0.99      0.95       434\n",
      "                          Strawberry___Leaf_scorch       0.93      0.92      0.92       444\n",
      "                              Strawberry___healthy       0.94      0.88      0.91       456\n",
      "                           Tomato___Bacterial_spot       0.80      0.93      0.86       425\n",
      "                             Tomato___Early_blight       0.83      0.69      0.75       480\n",
      "                              Tomato___Late_blight       0.77      0.72      0.75       463\n",
      "                                Tomato___Leaf_Mold       0.88      0.78      0.83       470\n",
      "                       Tomato___Septoria_leaf_spot       0.81      0.58      0.68       436\n",
      "     Tomato___Spider_mites Two-spotted_spider_mite       0.69      0.93      0.79       435\n",
      "                              Tomato___Target_Spot       0.78      0.72      0.75       457\n",
      "            Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.85      0.94      0.89       490\n",
      "                      Tomato___Tomato_mosaic_virus       0.93      0.93      0.93       448\n",
      "                                  Tomato___healthy       0.96      0.90      0.93       481\n",
      "\n",
      "                                         micro avg       0.87      0.87      0.87     17572\n",
      "                                         macro avg       0.87      0.87      0.87     17572\n",
      "                                      weighted avg       0.87      0.87      0.87     17572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "# model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=False\n",
    "                                                    )\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())   \n",
    "\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "import functools\n",
    "from matplotlib import pyplot\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn import metrics\n",
    "from keras.metrics import categorical_accuracy\n",
    "os.chdir(\"C:/Users/jerem.DESKTOP-GGM6Q2I/Documents/UNH Data Analytics/NN\")\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, LeakyReLU, Dropout, GaussianNoise\n",
    "from tensorflow import set_random_seed\n",
    "from keras.optimizers import Adam\n",
    "from keras_gradient_noise import add_gradient_noise\n",
    "from keras.initializers import glorot_uniform, glorot_normal, he_uniform, he_normal\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "set_random_seed(1724)\n",
    "shape=128\n",
    "\n",
    "batch_size = 128   \n",
    "train_dir = \"C:/Project/128/train\" \n",
    "test_dir = \"C:/Project/128/valid\"  \n",
    "nb_train_samples = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
    "nb_test_samples = sum([len(files) for r, d, files in os.walk(test_dir)])\n",
    "epochs = 150\n",
    "\n",
    "\n",
    "\n",
    "# base_model=VGG16(weights=\"imagenet\", include_top=False, input_shape=(shape,shape,3))\n",
    "# base_model.summary()\n",
    "# base_model = InceptionV3(weights=\"imagenet\", include_top=False)\n",
    "# base_model.summary()\n",
    "\n",
    "\n",
    "base_model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(shape,shape,3))\n",
    "x = base_model.output\n",
    "\n",
    "# x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(16, kernel_size = (3,3), strides = (1,1), padding = 'same', input_shape = (shape,shape,3)))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "# x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(32, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(64, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(128, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "# x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(Conv2D(256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\n",
    "# x.add(LeakyReLU())\n",
    "# x.add(BatchNormalization())\n",
    "# x.add(MaxPooling2D(pool_size = (2,2), strides = (2,2), padding = 'valid'))\n",
    "\n",
    "x = (Flatten())(x)\n",
    "x = (Dense(1024, kernel_initializer=\"glorot_normal\"))(x)\n",
    "x = (LeakyReLU())(x)\n",
    "x = (BatchNormalization())(x)\n",
    "\n",
    "\n",
    "pred = (Dense(38, activation='softmax'))(x)\n",
    "model = Model(inputs=base_model.input, outputs=pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy = add_gradient_noise(Adam)\n",
    "model.compile(optimizer=noisy(),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=False, vertical_flip=False, zca_whitening=True, zca_epsilon=.2, width_shift_range=0.2, height_shift_range=0, rotation_range=0, brightness_range=(0,1), shear_range=0, zoom_range=0.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) #don't add anything else to this one, only train datagen\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, \n",
    "                                                   target_size=(shape,shape),\n",
    "                                                   color_mode=\"rgb\",\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle=True)\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\"\n",
    "                                                    )\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=nb_train_samples // batch_size+1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=nb_test_samples // batch_size+1,\n",
    "                    workers=4,\n",
    "                    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17572 images belonging to 38 classes.\n",
      "                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                Apple___Apple_scab       0.84      0.79      0.81       504\n",
      "                                 Apple___Black_rot       0.88      0.85      0.87       497\n",
      "                          Apple___Cedar_apple_rust       0.91      0.84      0.87       440\n",
      "                                   Apple___healthy       0.90      0.79      0.84       502\n",
      "                               Blueberry___healthy       0.94      0.78      0.85       454\n",
      "          Cherry_(including_sour)___Powdery_mildew       0.86      0.95      0.90       421\n",
      "                 Cherry_(including_sour)___healthy       0.89      0.97      0.93       456\n",
      "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot       0.90      0.77      0.83       410\n",
      "                       Corn_(maize)___Common_rust_       0.94      0.99      0.96       477\n",
      "               Corn_(maize)___Northern_Leaf_Blight       0.90      0.91      0.91       477\n",
      "                            Corn_(maize)___healthy       0.99      0.99      0.99       465\n",
      "                                 Grape___Black_rot       0.87      0.86      0.87       472\n",
      "                      Grape___Esca_(Black_Measles)       0.90      0.90      0.90       480\n",
      "        Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       0.94      0.96      0.95       430\n",
      "                                   Grape___healthy       0.97      0.93      0.95       423\n",
      "          Orange___Haunglongbing_(Citrus_greening)       0.92      0.96      0.94       503\n",
      "                            Peach___Bacterial_spot       0.85      0.87      0.86       459\n",
      "                                   Peach___healthy       0.94      0.92      0.93       432\n",
      "                     Pepper,_bell___Bacterial_spot       0.64      0.91      0.75       478\n",
      "                            Pepper,_bell___healthy       0.71      0.82      0.76       497\n",
      "                             Potato___Early_blight       0.94      0.96      0.95       485\n",
      "                              Potato___Late_blight       0.81      0.87      0.84       485\n",
      "                                  Potato___healthy       0.90      0.86      0.88       456\n",
      "                               Raspberry___healthy       0.82      0.94      0.88       445\n",
      "                                 Soybean___healthy       0.99      0.72      0.83       505\n",
      "                           Squash___Powdery_mildew       0.91      0.99      0.95       434\n",
      "                          Strawberry___Leaf_scorch       0.93      0.92      0.92       444\n",
      "                              Strawberry___healthy       0.94      0.88      0.91       456\n",
      "                           Tomato___Bacterial_spot       0.80      0.93      0.86       425\n",
      "                             Tomato___Early_blight       0.83      0.69      0.75       480\n",
      "                              Tomato___Late_blight       0.77      0.72      0.75       463\n",
      "                                Tomato___Leaf_Mold       0.88      0.78      0.83       470\n",
      "                       Tomato___Septoria_leaf_spot       0.81      0.58      0.68       436\n",
      "     Tomato___Spider_mites Two-spotted_spider_mite       0.69      0.93      0.79       435\n",
      "                              Tomato___Target_Spot       0.78      0.72      0.75       457\n",
      "            Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.85      0.94      0.89       490\n",
      "                      Tomato___Tomato_mosaic_virus       0.93      0.93      0.93       448\n",
      "                                  Tomato___healthy       0.96      0.90      0.93       481\n",
      "\n",
      "                                         micro avg       0.87      0.87      0.87     17572\n",
      "                                         macro avg       0.87      0.87      0.87     17572\n",
      "                                      weighted avg       0.87      0.87      0.87     17572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5', custom_objects={\"NoisyAdam\":noisy()})\n",
    "# model.evaluate_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                    target_size=(shape,shape),\n",
    "                                                    color_mode=\"rgb\",\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle=False\n",
    "                                                    )\n",
    "\n",
    "predictions = model.predict_generator(test_generator, steps=nb_test_samples // batch_size+1, verbose=0)\n",
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())   \n",
    "\n",
    "report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
